{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_wiki_1 ='https://en.wikipedia.org/wiki/Pittsburgh'\n",
    "history_wiki_2 ='https://en.wikipedia.org/wiki/History_of_Pittsburgh'\n",
    "more =          'https://en.wikipedia.org/wiki/List_of_museums_in_Pittsburgh'\n",
    "\n",
    "pit_gov =       ['https://pittsburghpa.gov/index.html']\n",
    "brittanics =    ['https://www.britannica.com/place/Pittsburgh']\n",
    "visit_pit =     ['https://www.visitpittsburgh.com/']\n",
    "tax_reg =       ['https://pittsburghpa.gov/finance/tax-forms']\n",
    "op_budget =     ['https://apps.pittsburghpa.gov/redtail/images/23255_2024_Operating_Budget.pdf']\n",
    "about_cmu =     ['https://www.cmu.edu/about/']\n",
    "event_cal =     ['https://pittsburgh.events/']\n",
    "downtown_cal =  ['https://downtownpittsburgh.com/events/']\n",
    "city_paper =    ['https://www.pghcitypaper.com/pittsburgh/EventSearch?v=d']\n",
    "cmu_events =    ['https://events.cmu.edu/']\n",
    "campus_events = ['https://www.cmu.edu/engage/alumni/events/campus/index.html']\n",
    "symphony =      ['https://www.pittsburghsymphony.org/']\n",
    "opera =         ['https://pittsburghopera.org/']\n",
    "cultural_trust =['https://trustarts.org/']\n",
    "carn_museum =   ['https://carnegiemuseums.org/']\n",
    "heinz_museum =  ['https://www.heinzhistorycenter.org/']\n",
    "frick_museum =  ['https://www.thefrickpittsburgh.org/']\n",
    "food_fest =     ['https://www.visitpittsburgh.com/events-festivals/food-festivals/']\n",
    "pickle =        ['https://www.picklesburgh.com/']\n",
    "taco_fest =     ['https://www.pghtacofest.com/']\n",
    "restaurant_w =  ['https://pittsburghrestaurantweek.com/']\n",
    "little_italy =  ['https://littleitalydays.com/']\n",
    "banana_split =  ['https://bananasplitfest.com/']\n",
    "visit_pitsb  =  ['https://www.visitpittsburgh.com/things-to-do/pittsburgh-sports-teams/']\n",
    "pirates =       ['https://www.mlb.com/pirates']\n",
    "steelers =      ['https://www.steelers.com/']\n",
    "penguins =      ['https://www.nhl.com/penguins/']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/gman/anaconda3/envs/tensorflow/lib/python3.10/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/gman/anaconda3/envs/tensorflow/lib/python3.10/site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in /Users/gman/anaconda3/envs/tensorflow/lib/python3.10/site-packages (2.1.3)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/gman/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/gman/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/gman/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gman/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/gman/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/gman/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/gman/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/gman/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/gman/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/gman/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done scraping, saved to data/history_wiki_1.txt.\n",
      "Done scraping, saved to data/history_wiki_2.txt.\n",
      "Done scraping, saved to data/more.txt.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "#strip white spaces and new lines\n",
    "def clean_text(text):\n",
    "    return ' '.join(text.split())\n",
    "#truncate text\n",
    "def truncate(text, max_len):\n",
    "    return text[:max_len]\n",
    "\n",
    "# format table manually \n",
    "def format_table_manually(table_html, max_col_width=30):\n",
    "    EXCLUDE_PHRASES = [\n",
    "        \"This section does not cite any\",\n",
    "        \"This section needs additional\",\n",
    "        \"This section needs expansion\",\n",
    "    ]\n",
    "\n",
    "    rows = table_html.find_all('tr')\n",
    "    table_data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all(['th', 'td'])  \n",
    "        cols = [clean_text(col.get_text()) for col in cols]\n",
    "        if len(cols) > 1: \n",
    "            table_data.append(cols)\n",
    "\n",
    "    if not table_data:\n",
    "        return \"\"\n",
    "\n",
    "    for row in table_data:\n",
    "        for cell in row:\n",
    "            for phrase in EXCLUDE_PHRASES:\n",
    "                if phrase in cell:\n",
    "                    return \"\"\n",
    "\n",
    "    num_cols = max(len(row) for row in table_data)\n",
    "    non_empty_columns = [False] * num_cols\n",
    "\n",
    "    # non-empty columns\n",
    "    for row in table_data:\n",
    "        for i in range(num_cols):\n",
    "            if i < len(row) and row[i].strip():\n",
    "                non_empty_columns[i] = True\n",
    "\n",
    "    columns_to_keep = [i for i, has_content in enumerate(non_empty_columns) if has_content]\n",
    "\n",
    "    if not columns_to_keep:\n",
    "        return \"\"\n",
    "\n",
    "    # Remove empty columns from table_data\n",
    "    new_table_data = []\n",
    "    for row in table_data:\n",
    "        new_row = [row[i] if i < len(row) else '' for i in columns_to_keep]\n",
    "        new_table_data.append(new_row)\n",
    "\n",
    "    table_data = new_table_data\n",
    "    num_cols = len(columns_to_keep)\n",
    "\n",
    "    # dynamic column width calculation\n",
    "    col_widths = [0] * num_cols\n",
    "    for row in table_data:\n",
    "        for i, col in enumerate(row):\n",
    "            col_length = min(len(col), max_col_width)  # Cap column width at max_col_width\n",
    "            col_widths[i] = max(col_widths[i], col_length)\n",
    "\n",
    "    # format table and pad it correclty\n",
    "    table_str = \"\"\n",
    "    border_line = \"+\" + \"+\".join([\"-\" * width for width in col_widths]) + \"+\\n\"\n",
    "    table_str += border_line  \n",
    "\n",
    "    for row in table_data:\n",
    "        formatted_row = \"|\".join(f\"{truncate(col, col_widths[i]):<{col_widths[i]}}\" for i, col in enumerate(row))\n",
    "        table_str += f\"|{formatted_row}|\\n\"\n",
    "        table_str += border_line  \n",
    "\n",
    "    return table_str.strip()\n",
    "\n",
    "def scrape_wikipedia_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    EXCLUDED_SECTIONS = [\n",
    "        \"See also\",\n",
    "        \"Explanatory notes\",\n",
    "        \"References\",\n",
    "        \"Further reading\",\n",
    "        \"External links\"\n",
    "    ]\n",
    "\n",
    "    content = soup.find('div', {'id': 'bodyContent'})\n",
    "    sections = content.find_all(['h1', 'h2', 'h3', 'p', 'table', 'ul', 'ol'])\n",
    "\n",
    "    output = \"\"\n",
    "    current_section = \"Intro\"\n",
    "    section_content = \"\"\n",
    "\n",
    "    for section in sections:\n",
    "        if section.name in ['h1', 'h2', 'h3']:\n",
    "            section_title = clean_text(section.get_text())\n",
    "\n",
    "            # Exclude useless sections\n",
    "            if section_title in EXCLUDED_SECTIONS:\n",
    "                if section_content.strip():\n",
    "                    output += f\"=section_start=\\n=section name=\\\"{current_section}\\\"\\n{section_content.strip()}\\n=section_end=\\n\"\n",
    "                current_section = None\n",
    "                section_content = \"\"\n",
    "                continue\n",
    "            else:\n",
    "                if section_content.strip():\n",
    "                    output += f\"=section_start=\\n=section name=\\\"{current_section}\\\"\\n{section_content.strip()}\\n=section_end=\\n\"\n",
    "                current_section = section_title\n",
    "                section_content = \"\"\n",
    "                continue\n",
    "\n",
    "        if current_section:\n",
    "            if section.name == 'p':\n",
    "                paragraph = clean_text(section.get_text())\n",
    "                section_content += f\"{paragraph}\\n\"\n",
    "            elif section.name in ['ul', 'ol']:\n",
    "                list_items = section.find_all('li')\n",
    "                for item in list_items:\n",
    "                    item_text = clean_text(item.get_text())\n",
    "                    section_content += f\"- {item_text}\\n\"\n",
    "            elif section.name == 'table':\n",
    "                table_content_str = format_table_manually(section)\n",
    "                if table_content_str:\n",
    "                    section_content += f\"=== Table ===\\n{table_content_str}\\n=== End of Table ===\\n\"\n",
    "\n",
    "    if current_section and section_content.strip():\n",
    "        output += f\"=section_start=\\n=section name=\\\"{current_section}\\\"\\n{section_content.strip()}\\n=section_end=\\n\"\n",
    "\n",
    "    return output.strip()\n",
    "\n",
    "def scrape_and_save(url_variable_name, url):\n",
    "    \"\"\"Scrape Wikipedia page and save content as 'data/{url_variable_name}.txt'.\"\"\"\n",
    "    scraped_content = scrape_wikipedia_page(url)\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    file_path = f\"data/{url_variable_name}.txt\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(scraped_content)\n",
    "\n",
    "    print(f\"Done scraping, saved to {file_path}.\")\n",
    "\n",
    "\n",
    "\n",
    "scrape_and_save(\"history_wiki_1\", history_wiki_1)\n",
    "scrape_and_save(\"history_wiki_2\", history_wiki_2)\n",
    "scrape_and_save(\"more\", more)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
