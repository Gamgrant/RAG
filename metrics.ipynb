{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk import word_tokenize\n",
    "import json\n",
    "from utils import write_jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(candidate, reference):\n",
    "    '''\n",
    "    candidate, reference: generated and ground-truth sentences\n",
    "    '''\n",
    "    weights = [\n",
    "         (1./2., 1./2.),\n",
    "         (1./3., 1./3., 1./3.),\n",
    "         (1./4., 1./4., 1./4., 1./4.)\n",
    "    ]\n",
    "    reference = reference.strip().split()\n",
    "    candidate = candidate.strip().split()\n",
    "    score = sentence_bleu([reference], candidate, weights=weights)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_output_path = '/Users/raunaksood/Desktop/RAG/deepseek7B_results_bleu.jsonl'\n",
    "save_path = '/Users/raunaksood/Desktop/RAG/deepseek7B_results_bleu_NEW.jsonl'\n",
    "with open(llama_output_path) as f:\n",
    "    llama_outputs = [json.loads(line) for line in f]\n",
    "\n",
    "responses = [llama_outputs[i]['response'] for i in range(len(llama_outputs))]\n",
    "answers = [llama_outputs[i]['answer'] for i in range(len(llama_outputs))]\n",
    "questions = [llama_outputs[i]['question'] for i in range(len(llama_outputs))]\n",
    "bleus = [llama_outputs[i]['bleu'] for i in range(len(llama_outputs))]\n",
    "\n",
    "res = []\n",
    "for i in range(len(responses)):\n",
    "    question = questions[i]\n",
    "    response = responses[i]\n",
    "    answer = answers[i]\n",
    "    bleu = bleus[i]\n",
    "    match = response == answer\n",
    "    res.append({'question' : question, 'response' : response, 'answer' : answer, 'bleu' : bleu, 'match' : match})\n",
    "\n",
    "    \n",
    "write_jsonl(save_path, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F-1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following this paper: SQuAD: 100,000+ Questions for Machine Comprehension of Text\n",
    "#  P = num of correct tokens in predicted answer/ total tokens in predicted anser\n",
    "#  R = num of correct tokens in predicted answer/ total tokens in correct answer\n",
    "# F1 score = 2 * (P*R)/(P+R)\n",
    "\n",
    "# Approach 1 - sliding window (lower f1 score) - DONE\n",
    "# Approach 2 - logistic regression (higher f1 score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1 - sliding window (lower f1 score)\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(\\w)-(\\w)', r'\\1\\2', text)  \n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  \n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def f1_score_per_question(true_ans, pred_ans):\n",
    "    common = len(set(true_ans) & set(pred_ans))\n",
    "    P = common / len(pred_ans) if len(pred_ans) > 0 else 0  \n",
    "    R = common / len(true_ans) if len(true_ans) > 0 else 0  \n",
    "    if P + R == 0:\n",
    "        return 0, R, common, len(pred_ans), len(true_ans)  \n",
    "    f1_score = 2 * (P * R) / (P + R)\n",
    "    return f1_score, R, common, len(pred_ans), len(true_ans)\n",
    "\n",
    "def calculate_f1_for_file(file_path):\n",
    "    total_f1_score = 0\n",
    "    total_recall = 0\n",
    "    num_questions = 0\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            correct_answer = data['answer']\n",
    "            predicted_response = data['response']\n",
    "            correct_answer_tokens = tokenize(correct_answer)\n",
    "            response_tokens = tokenize(predicted_response)\n",
    "            f1, recall, overlap, total_predicted, total_correct = f1_score_per_question(correct_answer_tokens, response_tokens)\n",
    "            total_f1_score += f1\n",
    "            total_recall += recall\n",
    "            num_questions += 1\n",
    "\n",
    "    average_f1_score = total_f1_score / num_questions if num_questions > 0 else 0\n",
    "    average_recall = total_recall / num_questions if num_questions > 0 else 0\n",
    "    return average_f1_score, average_recall\n",
    "\n",
    "def calculate_f1_for_files(file_paths):\n",
    "    results_dict = {}\n",
    "    for file_path in file_paths:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        f1_score, recall = calculate_f1_for_file(file_path)\n",
    "        results_dict[file_name] = {'F1 Score': f1_score, 'Recall': recall}\n",
    "    return results_dict\n",
    "\n",
    "\n",
    "file_paths = ['deepseek7B_results.jsonl', 'llama3_8B_results.jsonl', 'mistral7B_results.jsonl']  # Replace with your file paths\n",
    "results = calculate_f1_for_files(file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek7B_results.jsonl:\n",
      "  F1 Score: 0.26\n",
      "  Recall: 0.48\n",
      "llama3_8B_results.jsonl:\n",
      "  F1 Score: 0.28\n",
      "  Recall: 0.59\n",
      "mistral7B_results.jsonl:\n",
      "  F1 Score: 0.24\n",
      "  Recall: 0.40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for file_name, scores in results.items():\n",
    "    print(f\"{file_name}:\")\n",
    "    print(f\"  F1 Score: {scores['F1 Score']:.2f}\")\n",
    "    print(f\"  Recall: {scores['Recall']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IAA (Grant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select random 50 questions\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "def create_subset(file_path, selected_indices):\n",
    "    new_data = []\n",
    "    file_name = os.path.basename(file_path)\n",
    "    new_file_name = file_name.replace('.jsonl', '_subset_grant_iaa.jsonl') \n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "        for i in selected_indices:\n",
    "            data = json.loads(lines[i])            \n",
    "            new_data_entry = {\n",
    "                \"question\": data[\"question\"],\n",
    "                \"response\": data[\"response\"],\n",
    "                \"answer\": \"\",   \n",
    "                \"bleu\": [],     \n",
    "                \"match\": None  \n",
    "            }\n",
    "            new_data.append(new_data_entry)\n",
    "\n",
    "    new_file_path = os.path.join(os.getcwd(), new_file_name)\n",
    "    with open(new_file_path, 'w') as new_file:\n",
    "        for entry in new_data:\n",
    "            new_file.write(json.dumps(entry) + '\\n')\n",
    "def subsets(file_paths):\n",
    "    with open(file_paths[0], 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        num_lines = len(lines)\n",
    "        selected_indices =  random.sample(range(num_lines), 50)\n",
    "    for file_path in file_paths:\n",
    "        create_subset(file_path, selected_indices)\n",
    "    return selected_indices\n",
    "files = ['deepseek7B_results.jsonl', 'llama3_8B_results.jsonl', 'mistral7B_results.jsonl']  \n",
    "selected_indices = subsets(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "answers =  [\"East Carson Street\",\n",
    "            \"Gene Kelly\",\n",
    "            \"1758\",\n",
    "            \"Forbes Field\",\n",
    "            \"Monongahela Incline Funicular, the T\",\n",
    "            \"In 1982\",\n",
    "            \"Primanti Brothers\",\n",
    "            \"My heart is in the work\",\n",
    "            \"2016\",\n",
    "            \"professor Scott Fahlman\",\n",
    "            \"The Kraft Heinz Company\",\n",
    "            \"William Pitt, 1st Earl of Chatham\",\n",
    "            \"Pittsburghese, or Yinzer\",\n",
    "            \"Bloomfield\",\n",
    "            \"Anti-Flag\",\n",
    "            \"General John Forbes\",\n",
    "            \"MJ the Musical\",\n",
    "            \"PNC\",\n",
    "            \"Buggy\",\n",
    "            \"Kennywood\",\n",
    "            \"Edward Manning Bigelow\",\n",
    "            \"Pirates or Pittsburgh Pirates\",\n",
    "            \"1979\", \n",
    "            \"Linda Babcock\",\n",
    "            \"7th best Drama School\",\n",
    "            \"The Pitt News or The Pitt Weekly\",\n",
    "            \"$7 billion\",\n",
    "            \"Pittsburgh Irish Festival\",\n",
    "            \"November 19th, 2024\",\n",
    "            \"In 1991\",\n",
    "            \"WYEP Summer Music Festival, Pittsburgh International Jazz Festival\",\n",
    "            \"Scotty or Scottish Terrier\",\n",
    "            \"The Mellon Institute\",\n",
    "            \"1979\",\n",
    "            \"Smithfield Street Bridge\",\n",
    "            \"#2 in undergraduate Computer Science\",\n",
    "            \"Point Breeze\",\n",
    "            \"Mister Rogers' Neighborhood\",\n",
    "            \"City of bridges\",\n",
    "            \"In 1983\",\n",
    "            \"After Roberto Clemente Bridge\",\n",
    "            \"Heinz Hall for the Performing Arts\",\n",
    "            \"Pennsylvania German\",\n",
    "            \"Fred Rogers\",\n",
    "            \"Carnegie Steel Company\",\n",
    "            \"McKees Rocks Bridge\",\n",
    "            \"The Baltimore Ravens\",\n",
    "            \"New York Giants\",\n",
    "            \"April 3rd - 5th 2025\",\n",
    "            \"Boston\"]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gman/anaconda3/envs/tensorflow/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/gman/anaconda3/envs/tensorflow/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/gman/anaconda3/envs/tensorflow/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.strip().lower()  \n",
    "    text = re.sub(r'[^a-z0-9]', '', text)  \n",
    "    return text\n",
    "\n",
    "def update_answers_in_files(file_paths, answers):\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines() \n",
    "\n",
    "        updated_data = []        \n",
    "        for i, line in enumerate(lines):\n",
    "            data = json.loads(line)\n",
    "            answer = answers[i]\n",
    "            data['answer'] = answer\n",
    "            response = data['response']\n",
    "            bleu_score = calculate_bleu(response, answer)\n",
    "            data['bleu'] = bleu_score\n",
    "            \n",
    "            cleaned_response = clean_text(response)\n",
    "            cleaned_answer = clean_text(answer)\n",
    "            data['match'] = cleaned_response == cleaned_answer\n",
    "            \n",
    "            updated_data.append(data)\n",
    "        with open(file_path, 'w') as file:\n",
    "            for entry in updated_data:\n",
    "                file.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "files = ['deepseek7B_results_subset_grant_iaa.jsonl', 'llama3_8B_results_subset_grant_iaa.jsonl', 'mistral7B_results_subset_grant_iaa.jsonl'] \n",
    "update_answers_in_files(files, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek7B_results_subset_grant_iaa.jsonl:\n",
      "  F1 Score: 0.12\n",
      "  Recall: 0.41\n",
      "llama3_8B_results_subset_grant_iaa.jsonl:\n",
      "  F1 Score: 0.14\n",
      "  Recall: 0.45\n",
      "mistral7B_results_subset_grant_iaa.jsonl:\n",
      "  F1 Score: 0.13\n",
      "  Recall: 0.32\n"
     ]
    }
   ],
   "source": [
    "files = ['deepseek7B_results_subset_grant_iaa.jsonl', 'llama3_8B_results_subset_grant_iaa.jsonl', 'mistral7B_results_subset_grant_iaa.jsonl'] \n",
    "results_iaa_grant = calculate_f1_for_files(files)\n",
    "for file_name, scores in results_iaa_grant.items():\n",
    "    print(f\"{file_name}:\")\n",
    "    print(f\"  F1 Score: {scores['F1 Score']:.2f}\")\n",
    "    print(f\"  Recall: {scores['Recall']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now evaluae metrics for RAG results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add those fields:\n",
    "# \"answer\": \"\",   \n",
    "# \"bleu\": [],     \n",
    "# \"match\": None \n",
    "import json\n",
    "\n",
    "def add_fields(file_path):\n",
    "    with open(file_path, 'r') as infile:\n",
    "        lines = infile.readlines()\n",
    "    with open(file_path, 'w') as outfile:\n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            data[\"answer\"] = \"\"\n",
    "            data[\"bleu\"] = []\n",
    "            data[\"match\"] = None\n",
    "            outfile.write(json.dumps(data) + '\\n')\n",
    "\n",
    "file_path = 'mistral7B_results_with_rag.jsonl' \n",
    "add_fields(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated mistral7B_results_with_rag.jsonl with answers, BLEU scores, and match results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gman/anaconda3/envs/tensorflow/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/gman/anaconda3/envs/tensorflow/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/gman/anaconda3/envs/tensorflow/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# now copy ground truth and compute bleu and exact match\n",
    "def copy_ans_compute_metrics(source_file, target_file):\n",
    "   \n",
    "    with open(source_file, 'r') as source_f, open(target_file, 'r') as target_f:\n",
    "        source_lines = source_f.readlines()  \n",
    "        target_lines = target_f.readlines()  \n",
    "    \n",
    "   \n",
    "    updated_data = []\n",
    "    for index, (source_line, target_line) in enumerate(zip(source_lines, target_lines)):\n",
    "        source_data = json.loads(source_line)\n",
    "        target_data = json.loads(target_line)\n",
    "        answer = source_data[\"answer\"]\n",
    "        target_data[\"answer\"] = answer\n",
    "        response = target_data['response']  \n",
    "        bleu_score = calculate_bleu(response, answer)\n",
    "        target_data['bleu'] = bleu_score\n",
    "        cleaned_response = clean_text(response)\n",
    "        cleaned_answer = clean_text(answer)\n",
    "        target_data['match'] = cleaned_response == cleaned_answer\n",
    "        updated_data.append(json.dumps(target_data))\n",
    "\n",
    "    with open(target_file, 'w') as target_f:\n",
    "        for line in updated_data:\n",
    "            target_f.write(line + '\\n')\n",
    "\n",
    "    print(f\"Updated {target_file} with answers, BLEU scores, and match results.\")\n",
    "\n",
    "\n",
    "source_file = 'mistral7B_results.jsonl'  \n",
    "target_file = 'mistral7B_results_with_rag.jsonl'  \n",
    "copy_ans_compute_metrics(source_file, target_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistral7B_results_with_rag.jsonl:\n",
      "  F1 Score: 0.41\n",
      "  Recall: 0.52\n"
     ]
    }
   ],
   "source": [
    "results_iaa_grant = calculate_f1_for_files([target_file])\n",
    "for file_name, scores in results_iaa_grant.items():\n",
    "    print(f\"{file_name}:\")\n",
    "    print(f\"  F1 Score: {scores['F1 Score']:.2f}\")\n",
    "    print(f\"  Recall: {scores['Recall']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
