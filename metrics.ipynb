{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk import word_tokenize\n",
    "import json\n",
    "from utils import write_jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(candidate, reference):\n",
    "    '''\n",
    "    candidate, reference: generated and ground-truth sentences\n",
    "    '''\n",
    "    weights = [\n",
    "         (1./2., 1./2.),\n",
    "         (1./3., 1./3., 1./3.),\n",
    "         (1./4., 1./4., 1./4., 1./4.)\n",
    "    ]\n",
    "    reference = reference.strip().split()\n",
    "    candidate = candidate.strip().split()\n",
    "    score = sentence_bleu([reference], candidate, weights=weights)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_output_path = '/Users/raunaksood/Desktop/RAG/deepseek7B_results_bleu.jsonl'\n",
    "save_path = '/Users/raunaksood/Desktop/RAG/deepseek7B_results_bleu_NEW.jsonl'\n",
    "with open(llama_output_path) as f:\n",
    "    llama_outputs = [json.loads(line) for line in f]\n",
    "\n",
    "responses = [llama_outputs[i]['response'] for i in range(len(llama_outputs))]\n",
    "answers = [llama_outputs[i]['answer'] for i in range(len(llama_outputs))]\n",
    "questions = [llama_outputs[i]['question'] for i in range(len(llama_outputs))]\n",
    "bleus = [llama_outputs[i]['bleu'] for i in range(len(llama_outputs))]\n",
    "\n",
    "res = []\n",
    "for i in range(len(responses)):\n",
    "    question = questions[i]\n",
    "    response = responses[i]\n",
    "    answer = answers[i]\n",
    "    bleu = bleus[i]\n",
    "    match = response == answer\n",
    "    res.append({'question' : question, 'response' : response, 'answer' : answer, 'bleu' : bleu, 'match' : match})\n",
    "\n",
    "    \n",
    "write_jsonl(save_path, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F-1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following this paper: SQuAD: 100,000+ Questions for Machine Comprehension of Text\n",
    "#  P = num of correct tokens in predicted answer/ total tokens in predicted anser\n",
    "#  R = num of correct tokens in predicted answer/ total tokens in correct answer\n",
    "# F1 score = 2 * (P*R)/(P+R)\n",
    "\n",
    "# Approach 1 - sliding window (lower f1 score) - DONE\n",
    "# Approach 2 - logistic regression (higher f1 score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1 - sliding window (lower f1 score)\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(\\w)-(\\w)', r'\\1\\2', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def f1_score_per_question(true_ans, pred_ans):\n",
    "    common = len(set(true_ans) & set(pred_ans))\n",
    "    P = common / len(pred_ans) if len(pred_ans) > 0 else 0\n",
    "    R = common / len(true_ans) if len(true_ans) > 0 else 0\n",
    "    if P + R == 0:\n",
    "        return 0, common, len(pred_ans), len(true_ans)\n",
    "    f1_score = 2 * (P * R) / (P + R)\n",
    "    return f1_score, common, len(pred_ans), len(true_ans)\n",
    "\n",
    "def calculate_f1_for_file(file_path):\n",
    "    total_f1_score = 0\n",
    "    num_questions = 0\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        # print(f\"json file name: {file_path}\\n\")  \n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            correct_answer = data['answer']\n",
    "            predicted_response = data['response']\n",
    "            correct_answer_tokens = tokenize(correct_answer)\n",
    "            response_tokens = tokenize(predicted_response)\n",
    "            f1, overlap, total_predicted, total_correct = f1_score_per_question(correct_answer_tokens, response_tokens)\n",
    "            total_f1_score += f1\n",
    "            num_questions += 1\n",
    "            # print(f\"Question: {data['question']}\")\n",
    "            # print(f\"Correct answer: {correct_answer_tokens}\")\n",
    "            # print(f\"Predicted anwer: {response_tokens}\")\n",
    "            # print(f\"Correct tokens in predicted answer: {overlap}\")\n",
    "            # print(f\"Total tokens in predicted answer: {total_predicted}\")\n",
    "            # print(f\"Total tokens in correct answer: {total_correct}\")\n",
    "            # print(f\"F1 Score: {f1:.2f}\\n\")\n",
    "    average_f1_score = total_f1_score / num_questions if num_questions > 0 else 0\n",
    "    return average_f1_score\n",
    "\n",
    "def calculate_f1_for_files(file_paths):\n",
    "    f1_scores_dict = {}\n",
    "    for file_path in file_paths:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        f1_score = calculate_f1_for_file(file_path)\n",
    "        f1_scores_dict[file_name] = f1_score  \n",
    "    return f1_scores_dict\n",
    "\n",
    "file_paths = ['deepseek7B_results.jsonl', 'llama3_8B_results.jsonl', 'mistral7B_results.jsonl']  \n",
    "f1_scores = calculate_f1_for_files(file_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Scores for each file:\n",
      "\tdeepseek7B_results.jsonl: 0.26\n",
      "\tllama3_8B_results.jsonl: 0.28\n",
      "\tmistral7B_results.jsonl: 0.24\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Scores for each file:\")\n",
    "for file_name, f1_score in f1_scores.items():\n",
    "    print(f\"\\t{file_name}: {f1_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
